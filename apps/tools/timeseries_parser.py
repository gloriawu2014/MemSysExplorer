#!/usr/bin/env python3
"""
Time-Series Protobuf Parser

Parses .pb (protobuf) time-series data files generated by profilers
and converts them to various output formats (JSON, CSV, or Python dict).

Usage:
    python timeseries_parser.py <input.pb> [--format json|csv|dict] [--output file]

Examples:
    # Print as JSON to stdout
    python timeseries_parser.py timeseries_ls_12345.pb --format json

    # Export to CSV file
    python timeseries_parser.py timeseries_ls_12345.pb --format csv --output output.csv

    # Filter by thread
    python timeseries_parser.py timeseries_ls_12345.pb --thread 12345 --format csv
"""

import sys
import os
import argparse
import json
import csv
from pathlib import Path

# Add the common/proto directory to path to import generated protobuf code
SCRIPT_DIR = Path(__file__).parent
PROTO_DIR = SCRIPT_DIR.parent / "profilers" / "common" / "proto"
sys.path.insert(0, str(PROTO_DIR))

try:
    # Import the generated protobuf module
    # This will be generated by: protoc --python_out=. timeseries_metrics.proto
    import timeseries_metrics_pb2 as ts_pb
except ImportError as e:
    print(f"Error: Could not import timeseries_metrics_pb2", file=sys.stderr)
    print(f"Please run: cd {PROTO_DIR} && protoc --python_out=. timeseries_metrics.proto", file=sys.stderr)
    sys.exit(1)


class TimeSeriesParser:
    """Parser for protobuf time-series data files"""

    def __init__(self, pb_file):
        """
        Initialize parser with a protobuf file

        Args:
            pb_file (str): Path to .pb file
        """
        self.pb_file = pb_file
        self.data = None
        self._load()

    def _load(self):
        """Load and parse the protobuf file (supports both single-message and length-delimited formats)"""
        if not os.path.exists(self.pb_file):
            raise FileNotFoundError(f"Protobuf file not found: {self.pb_file}")

        self.data = ts_pb.TimeSeriesData()

        try:
            with open(self.pb_file, 'rb') as f:
                file_content = f.read()

                # Check if file is empty
                if len(file_content) == 0:
                    raise ValueError("File is empty (0 bytes)")

                # Try parsing as length-delimited format first (new format with periodic flushing)
                if self._try_parse_length_delimited(file_content):
                    return

                # Fall back to single-message format (old format)
                self.data.ParseFromString(file_content)

        except Exception as e:
            raise ValueError(f"Failed to parse protobuf file: {e}")

    def _try_parse_length_delimited(self, content):
        """
        Try to parse length-delimited format where each message is prefixed with 4-byte length.
        Returns True if successful, False otherwise.
        """
        import struct

        try:
            offset = 0
            all_samples = []
            metadata = None

            while offset < len(content):
                # Read 4-byte message length
                if offset + 4 > len(content):
                    # Not enough bytes for length prefix
                    return False

                msg_size = struct.unpack('<I', content[offset:offset+4])[0]
                offset += 4

                # Check if message size is reasonable (< 1GB)
                if msg_size == 0 or msg_size > 1024*1024*1024:
                    return False

                # Read the message
                if offset + msg_size > len(content):
                    # Not enough bytes for message
                    return False

                msg_data = content[offset:offset+msg_size]
                offset += msg_size

                # Parse this chunk
                chunk = ts_pb.TimeSeriesData()
                chunk.ParseFromString(msg_data)

                # Collect metadata from first chunk
                if metadata is None and chunk.HasField('metadata'):
                    metadata = chunk.metadata

                # Collect all samples
                all_samples.extend(chunk.samples)

            # If we successfully parsed all chunks, reconstruct the data
            if metadata:
                self.data.metadata.CopyFrom(metadata)

            # Add all collected samples
            for sample in all_samples:
                new_sample = self.data.samples.add()
                new_sample.CopyFrom(sample)

            return True

        except Exception:
            # If anything fails, return False to try single-message format
            return False

    def to_dict(self):
        """
        Convert protobuf data to Python dictionary

        Returns:
            dict: Complete time-series data as dictionary
        """
        result = {
            'metadata': {
                'profiler': self.data.metadata.profiler,
                'pid': self.data.metadata.pid,
                'start_timestamp': self.data.metadata.start_timestamp,
                'command': self.data.metadata.command,
                'sample_window_refs': self.data.metadata.sample_window_refs,
                'cache_line_size': self.data.metadata.cache_line_size,
                'num_threads': self.data.metadata.num_threads
            },
            'samples': []
        }

        for sample in self.data.samples:
            result['samples'].append({
                'window_number': sample.window_number,
                'thread_id': sample.thread_id,
                'read_count': sample.read_count,
                'write_count': sample.write_count,
                'total_refs': sample.total_refs,
                'wss_exact': sample.wss_exact,
                'wss_approx': sample.wss_approx,
                'timestamp': sample.timestamp,
                'read_size_histogram': {
                    '1': sample.read_size_1,
                    '2': sample.read_size_2,
                    '4': sample.read_size_4,
                    '8': sample.read_size_8,
                    '16': sample.read_size_16,
                    '32': sample.read_size_32,
                    '64': sample.read_size_64,
                    'other': sample.read_size_other
                },
                'write_size_histogram': {
                    '1': sample.write_size_1,
                    '2': sample.write_size_2,
                    '4': sample.write_size_4,
                    '8': sample.write_size_8,
                    '16': sample.write_size_16,
                    '32': sample.write_size_32,
                    '64': sample.write_size_64,
                    'other': sample.write_size_other
                }
            })

        return result

    def to_json(self, indent=2):
        """
        Convert to JSON string

        Args:
            indent (int): JSON indentation level

        Returns:
            str: JSON formatted string
        """
        return json.dumps(self.to_dict(), indent=indent)

    def to_csv(self, output_file=None, filter_thread=None):
        """
        Convert to CSV format

        Args:
            output_file (str): Output CSV file path (None = stdout)
            filter_thread (int): Optional thread ID to filter

        Returns:
            str: CSV string if output_file is None
        """
        samples = self.data.samples

        # Filter by thread if requested
        if filter_thread is not None:
            samples = [s for s in samples if s.thread_id == filter_thread]

        # CSV header
        fieldnames = ['window_number', 'thread_id', 'read_count', 'write_count', 'total_refs',
                     'wss_exact', 'wss_approx', 'timestamp']

        if output_file:
            with open(output_file, 'w', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for sample in samples:
                    writer.writerow({
                        'window_number': sample.window_number,
                        'thread_id': sample.thread_id,
                        'read_count': sample.read_count,
                        'write_count': sample.write_count,
                        'total_refs': sample.total_refs,
                        'wss_exact': sample.wss_exact,
                        'wss_approx': sample.wss_approx,
                        'timestamp': sample.timestamp
                    })
        else:
            # Return as string
            import io
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()
            for sample in samples:
                writer.writerow({
                    'window_number': sample.window_number,
                    'thread_id': sample.thread_id,
                    'read_count': sample.read_count,
                    'write_count': sample.write_count,
                    'total_refs': sample.total_refs,
                    'wss_exact': sample.wss_exact,
                    'wss_approx': sample.wss_approx,
                    'timestamp': sample.timestamp
                })
            return output.getvalue()

    def get_summary(self):
        """
        Get summary statistics

        Returns:
            dict: Summary statistics
        """
        if not self.data.samples:
            return {"error": "No samples found"}

        total_reads = sum(s.read_count for s in self.data.samples)
        total_writes = sum(s.write_count for s in self.data.samples)
        threads = set(s.thread_id for s in self.data.samples)

        return {
            'profiler': self.data.metadata.profiler,
            'pid': self.data.metadata.pid,
            'command': self.data.metadata.command,
            'num_samples': len(self.data.samples),
            'num_threads': len(threads),
            'thread_ids': sorted(threads),
            'total_reads': total_reads,
            'total_writes': total_writes,
            'total_refs': total_reads + total_writes,
            'read_write_ratio': total_reads / total_writes if total_writes > 0 else float('inf'),
            'avg_wss_exact': sum(s.wss_exact for s in self.data.samples) / len(self.data.samples),
            'avg_wss_approx': sum(s.wss_approx for s in self.data.samples) / len(self.data.samples),
            'max_wss_exact': max(s.wss_exact for s in self.data.samples),
            'max_wss_approx': max(s.wss_approx for s in self.data.samples)
        }

    def filter_by_thread(self, thread_id):
        """
        Get samples for a specific thread

        Args:
            thread_id (int): Thread ID to filter

        Returns:
            list: List of samples for the thread
        """
        return [s for s in self.data.samples if s.thread_id == thread_id]


def main():
    parser = argparse.ArgumentParser(
        description='Parse protobuf time-series data files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument('input', help='Input .pb protobuf file')
    parser.add_argument('--format', choices=['json', 'csv', 'summary'],
                       default='summary', help='Output format')
    parser.add_argument('--output', '-o', help='Output file (default: stdout)')
    parser.add_argument('--thread', type=int, help='Filter by thread ID')
    parser.add_argument('--indent', type=int, default=2,
                       help='JSON indentation (default: 2)')

    args = parser.parse_args()

    try:
        # Parse the protobuf file
        ts_parser = TimeSeriesParser(args.input)

        # Generate output based on format
        if args.format == 'json':
            output = ts_parser.to_json(indent=args.indent)
        elif args.format == 'csv':
            output = ts_parser.to_csv(output_file=args.output, filter_thread=args.thread)
            if args.output:
                print(f"CSV written to {args.output}")
                return
        elif args.format == 'summary':
            output = json.dumps(ts_parser.get_summary(), indent=args.indent)

        # Write to file or stdout
        if args.output and args.format != 'csv':
            with open(args.output, 'w') as f:
                f.write(output)
            print(f"Output written to {args.output}")
        else:
            print(output)

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
