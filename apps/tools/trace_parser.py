#!/usr/bin/env python3
"""
Memory Trace Protobuf Parser

Parses .pb (protobuf) memory trace files generated by profilers
and converts them to various output formats (JSON, CSV, or Python dict).

Usage:
    python trace_parser.py <input.pb> [--format json|csv|dict] [--output file]

Examples:
    # Print as JSON to stdout
    python trace_parser.py memtrace_12345.pb --format json

    # Export to CSV file
    python trace_parser.py memtrace_12345.pb --format csv --output trace.csv

    # Filter by thread
    python trace_parser.py memtrace_12345.pb --thread 12345 --format csv

    # Limit number of events
    python trace_parser.py memtrace_12345.pb --limit 1000 --format csv
"""

import sys
import os
import argparse
import json
import csv
from pathlib import Path

# Add the common/proto directory to path to import generated protobuf code
SCRIPT_DIR = Path(__file__).parent
PROTO_DIR = SCRIPT_DIR.parent / "profilers" / "common" / "proto"
sys.path.insert(0, str(PROTO_DIR))

try:
    # Import the generated protobuf module
    import memory_trace_pb2 as trace_pb
except ImportError as e:
    print(f"Error: Could not import memory_trace_pb2", file=sys.stderr)
    print(f"Please run: cd {PROTO_DIR} && protoc --python_out=. memory_trace.proto", file=sys.stderr)
    sys.exit(1)


class MemoryTraceParser:
    """Parser for protobuf memory trace files"""

    def __init__(self, pb_file):
        """
        Initialize parser with a protobuf file

        Args:
            pb_file (str): Path to .pb file
        """
        self.pb_file = pb_file
        self.trace = None
        self._load()

    def _load(self):
        """Load and parse the protobuf file (supports both single-message and length-delimited formats)"""
        if not os.path.exists(self.pb_file):
            raise FileNotFoundError(f"Protobuf file not found: {self.pb_file}")

        self.trace = trace_pb.MemoryTrace()

        try:
            with open(self.pb_file, 'rb') as f:
                file_content = f.read()

                # Check if file is empty
                if len(file_content) == 0:
                    raise ValueError("File is empty (0 bytes)")

                # Try parsing as length-delimited format first (new format with periodic flushing)
                if self._try_parse_length_delimited(file_content):
                    return

                # Fall back to single-message format (old format)
                self.trace.ParseFromString(file_content)

        except Exception as e:
            raise ValueError(f"Failed to parse protobuf file: {e}")

    def _try_parse_length_delimited(self, content):
        """
        Try to parse length-delimited format where each message is prefixed with 4-byte length.
        Returns True if successful, False otherwise.
        """
        import struct

        try:
            offset = 0
            all_events = []

            while offset < len(content):
                # Read 4-byte message length
                if offset + 4 > len(content):
                    # Not enough bytes for length prefix
                    return False

                msg_size = struct.unpack('<I', content[offset:offset+4])[0]
                offset += 4

                # Check if message size is reasonable (< 1GB)
                if msg_size == 0 or msg_size > 1024*1024*1024:
                    return False

                # Read the message
                if offset + msg_size > len(content):
                    # Not enough bytes for message
                    return False

                msg_data = content[offset:offset+msg_size]
                offset += msg_size

                # Parse this chunk
                chunk = trace_pb.MemoryTrace()
                chunk.ParseFromString(msg_data)

                # Collect all events
                all_events.extend(chunk.events)

            # Add all collected events to the trace
            for event in all_events:
                new_event = self.trace.events.add()
                new_event.CopyFrom(event)

            return True

        except Exception:
            # If anything fails, return False to try single-message format
            return False

    def to_dict(self, limit=None, filter_thread=None):
        """
        Convert protobuf data to Python dictionary

        Args:
            limit (int): Maximum number of events to include
            filter_thread (int): Optional thread ID to filter

        Returns:
            dict: Complete trace data as dictionary
        """
        events = self.trace.events

        # Filter by thread if requested
        if filter_thread is not None:
            events = [e for e in events if e.thread_id == filter_thread]

        # Limit number of events
        if limit:
            events = events[:limit]

        result = {
            'num_events': len(events),
            'events': []
        }

        for event in events:
            result['events'].append({
                'timestamp': event.timestamp,
                'thread_id': event.thread_id,
                'address': hex(event.address),
                'mem_op': 'WRITE' if event.mem_op == trace_pb.WRITE else 'READ',
                'hit_miss': 'MISS' if event.hit_miss == trace_pb.MISS else 'HIT'
            })

        return result

    def to_json(self, indent=2, limit=None, filter_thread=None):
        """
        Convert to JSON string

        Args:
            indent (int): JSON indentation level
            limit (int): Maximum number of events
            filter_thread (int): Optional thread ID to filter

        Returns:
            str: JSON formatted string
        """
        return json.dumps(self.to_dict(limit=limit, filter_thread=filter_thread), indent=indent)

    def to_csv(self, output_file=None, limit=None, filter_thread=None):
        """
        Convert to CSV format

        Args:
            output_file (str): Output CSV file path (None = stdout)
            limit (int): Maximum number of events
            filter_thread (int): Optional thread ID to filter

        Returns:
            str: CSV string if output_file is None
        """
        events = self.trace.events

        # Filter by thread if requested
        if filter_thread is not None:
            events = [e for e in events if e.thread_id == filter_thread]

        # Limit number of events
        if limit:
            events = events[:limit]

        # CSV header
        fieldnames = ['timestamp', 'thread_id', 'address', 'mem_op', 'hit_miss']

        if output_file:
            with open(output_file, 'w', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for event in events:
                    writer.writerow({
                        'timestamp': event.timestamp,
                        'thread_id': event.thread_id,
                        'address': hex(event.address),
                        'mem_op': 'WRITE' if event.mem_op == trace_pb.WRITE else 'READ',
                        'hit_miss': 'MISS' if event.hit_miss == trace_pb.MISS else 'HIT'
                    })
        else:
            # Return as string
            import io
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()
            for event in events:
                writer.writerow({
                    'timestamp': event.timestamp,
                    'thread_id': event.thread_id,
                    'address': hex(event.address),
                    'mem_op': 'WRITE' if event.mem_op == trace_pb.WRITE else 'READ',
                    'hit_miss': 'MISS' if event.hit_miss == trace_pb.MISS else 'HIT'
                })
            return output.getvalue()

    def get_summary(self):
        """
        Get summary statistics

        Returns:
            dict: Summary statistics
        """
        if not self.trace.events:
            return {"error": "No events found"}

        total_reads = sum(1 for e in self.trace.events if e.mem_op == trace_pb.READ)
        total_writes = sum(1 for e in self.trace.events if e.mem_op == trace_pb.WRITE)
        threads = set(e.thread_id for e in self.trace.events)
        unique_addresses = len(set(e.address for e in self.trace.events))

        return {
            'total_events': len(self.trace.events),
            'num_threads': len(threads),
            'thread_ids': sorted(threads),
            'total_reads': total_reads,
            'total_writes': total_writes,
            'read_write_ratio': total_reads / total_writes if total_writes > 0 else float('inf'),
            'unique_addresses': unique_addresses,
            'timestamp_range': {
                'start': min(e.timestamp for e in self.trace.events),
                'end': max(e.timestamp for e in self.trace.events)
            }
        }

    def filter_by_thread(self, thread_id):
        """
        Get events for a specific thread

        Args:
            thread_id (int): Thread ID to filter

        Returns:
            list: List of events for the thread
        """
        return [e for e in self.trace.events if e.thread_id == thread_id]

    def filter_by_address_range(self, start_addr, end_addr):
        """
        Get events within an address range

        Args:
            start_addr (int): Start address
            end_addr (int): End address

        Returns:
            list: List of events in the range
        """
        return [e for e in self.trace.events
                if start_addr <= e.address <= end_addr]


def main():
    parser = argparse.ArgumentParser(
        description='Parse protobuf memory trace files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument('input', help='Input .pb protobuf file')
    parser.add_argument('--format', choices=['json', 'csv', 'summary'],
                       default='summary', help='Output format')
    parser.add_argument('--output', '-o', help='Output file (default: stdout)')
    parser.add_argument('--thread', type=int, help='Filter by thread ID')
    parser.add_argument('--limit', type=int, help='Limit number of events')
    parser.add_argument('--indent', type=int, default=2,
                       help='JSON indentation (default: 2)')

    args = parser.parse_args()

    try:
        # Parse the protobuf file
        trace_parser = MemoryTraceParser(args.input)

        # Generate output based on format
        if args.format == 'json':
            output = trace_parser.to_json(indent=args.indent,
                                         limit=args.limit,
                                         filter_thread=args.thread)
        elif args.format == 'csv':
            output = trace_parser.to_csv(output_file=args.output,
                                        limit=args.limit,
                                        filter_thread=args.thread)
            if args.output:
                print(f"CSV written to {args.output}")
                return
        elif args.format == 'summary':
            output = json.dumps(trace_parser.get_summary(), indent=args.indent)

        # Write to file or stdout
        if args.output and args.format != 'csv':
            with open(args.output, 'w') as f:
                f.write(output)
            print(f"Output written to {args.output}")
        else:
            print(output)

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
